<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WRPY5SCE0Y"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-WRPY5SCE0Y');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../style.css">

    <script src="https://www.w3school.com.cn/lib/bs/bootstrap.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;500&display=swap" rel="stylesheet">
    <title>Yuecheng</title>
</head>

<body>
    <div style="display: flex; justify-content: center;">
        <div class="col-lg-8">
            <header id="header" style="z-index: 99;">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-6" style="display: flex;">
                            <h3>OmniSurface <a class="h3italic taglink" href="../index.html">@ Yuecheng, Wanling, Yi</a>
                            </h3>
                        </div>
                        <div class="col-lg-6 headertagGroup">
                            <a class="headertag taglink" href="../index.html">‚Üê back to homepage</a>
                        </div>
                    </div>
                </div>
                <div style="margin-bottom: 1.2rem;"></div>

                <div class="border-top mb-5 horizontal-divider"></div>

            </header>

            <div class="row projectGallerySectionHeader">
                <div class="col-lg-6 profileText">
                    <div class="projectIntro">
                        <p>ü´≥ This is an IoT device capable of turning passive surfaces into interactive inputs.
                            It can sense different gestures on various surfaces, e.g.
                            knocking, tapping, fist bumping, etc. It can also sense different objects that are
                            interacting with the surface, e.g. interact with hands, pencils, mugs, etc.
                        </p>

                        <p class="underline">üîÜ Highlights</p>
                        <ul>
                            <li>
                                Highly integrated hardware design; software architecture fully functional and scalable
                                on the Azure Cloud.
                            </li>

                            <li>
                                Customized gestures: users can define their own gestures, and
                                add new data to the training dataset to customize their machine learning classifier.
                            </li>
                            <li>
                                We envision this novel input can be useful in promoting accessibility, e.g. people who
                                cannot perform accurate gestures on traditional tangible interfaces (e.g. rotate a knob)
                                might find it easier to interact with a larger surface with possible higher tolerance.
                                It also has potential applications in the gaming industry.
                            </li>
                        </ul>

                        <p class="underline">‚ö†Ô∏è Limitations</p>
                        <ul>
                            <li>
                                The accuracy of the classifier is highly dependent on what gestures users define and the
                                data size. More research should be done to improve the accuracy on a small training
                                dataset.
                            </li>
                            <li>
                                Research should also be done to optimize the hardware: do we need two different sensors?
                                Does adding a higher sampling rate and resolution ADC help to improve the ML accuracy?
                            </li>


                        </ul>

                        <p>üîó Github: <a class="taglink"
                                href="https://github.com/OmniSurface">https://github.com/OmniSurface</a>
                        </p>
                        <p>üçæ This is the final project of the TECHIN 515 course @ GIX, UW. This is a group project in
                            collaboration with Wanling and Yi. I was responsible for the
                            design concept iteration, hardware implementation and cloud service development.
                        </p>
                    </div>
                </div>
                <div class="col-lg-6 profileDiv">
                    <img class="projectTeaser" src="../img/omnisurface/teaser.png" alt="">
                </div>
            </div>

            <div class="row projectGallerySection">
                <div class="video-container">
                    <iframe src="https://www.youtube.com/embed/BvffxSwTvvw?vq=hd1080" frameborder="0"
                        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                </div>
            </div>

            <div class="row projectGallerySection">
                <div class="col-lg-4">
                    <p>
                        The software pipeline incorporates the control over a smart home device (a lamp with
                        different colors). Different gestures can be used to turn on/off different colors of the LED.
                    </p>
                </div>
                <div class="col-lg">
                    <img class="project_fullWidth" src="../img/omnisurface/smarthome_1.jpg" alt="">
                </div>
                <!-- <div class="col-lg-2"></div> -->
            </div>

            <div class="row projectGallerySection">
                <div class="col-lg">
                    <img class="project_fullWidth" src="../img/omnisurface/pcb.jpeg" alt="">
                </div>
                <div class="col-lg-4">
                    <p>
                        The combination use of piezoelectric sensors and bone conduction MEMS microphones can pick up
                        both obvious and subtle vibrations on the surface. The data collected from both sensors will be
                        amplified by a dual-channel amplifier and then fed into the ESP32S3, which will send the data to
                        the Azure Funcion App for further processing & classification.
                    </p>
                </div>
            </div>

            <div class="row projectGallerySection">
                <div class="col-lg-4">
                    <p>
                        The mockup smart home lamp is controlled by ESP32S3 and Blynk.
                    </p>
                </div>
                <div class="col-lg">
                    <img class="project_fullWidth" src="../img/omnisurface/lamp_PCB.png" alt="">
                </div>
            </div>

            <div class="border-top mt-5 mb-3 horizontal-divider"></div>
            <footer id="footer">
                <div class="container">
                    <div class="row mb-5">
                        <p style="text-align: center;">Copyright 2024 Yuecheng Peng</p>
                    </div>
                </div>
            </footer>

        </div>
    </div>

</body>

</html>